{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import imageio"
      ],
      "metadata": {
        "id": "frOIQ0_u3HG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATASET_DIR = \"/content/drive/MyDrive/chest_xray\"\n",
        "\n",
        "import os, shutil, random, glob, zipfile, pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "assert os.path.exists(DATASET_DIR), f\"Path not found: {DATASET_DIR}\"\n",
        "print(\"Using dataset root:\", DATASET_DIR)\n"
      ],
      "metadata": {
        "id": "7Fttm3m93Kl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# ---- CBAM MODULES ----\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.mlp(self.avg_pool(x))\n",
        "        max_out = self.mlp(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        assert kernel_size in (3, 7)\n",
        "        padding = 3 if kernel_size == 7 else 1\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x_cat)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "\n",
        "class CBAMBlock(nn.Module):\n",
        "    def __init__(self, channels, ratio=16, kernel_size=7):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(channels, ratio)\n",
        "        self.sa = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ca(x) * x\n",
        "        out = self.sa(out) * out\n",
        "        return out\n",
        "\n",
        "\n",
        "# ---- RESNET18 + CBAM WRAPPER ----\n",
        "class ResNet18_CBAM(nn.Module):\n",
        "    def __init__(self, num_classes=2, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.resnet18(\n",
        "            weights=models.ResNet18_Weights.DEFAULT if pretrained else None\n",
        "        )\n",
        "\n",
        "        # Insert a CBAM block after the last residual block (layer4)\n",
        "        # layer4 output has 512 channels in ResNet18\n",
        "        self.backbone = base\n",
        "        self.cbam = CBAMBlock(512)\n",
        "\n",
        "        # Replace classifier to match num_classes\n",
        "        in_features = base.fc.in_features\n",
        "        self.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Same forward as ResNet18 until layer4\n",
        "        x = self.backbone.conv1(x)\n",
        "        x = self.backbone.bn1(x)\n",
        "        x = self.backbone.relu(x)\n",
        "        x = self.backbone.maxpool(x)\n",
        "\n",
        "        x = self.backbone.layer1(x)\n",
        "        x = self.backbone.layer2(x)\n",
        "        x = self.backbone.layer3(x)\n",
        "        x = self.backbone.layer4(x)   # [B,512,H,W]\n",
        "\n",
        "        # Apply CBAM attention over layer4 feature map\n",
        "        x = self.cbam(x)\n",
        "\n",
        "        x = self.backbone.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "pi7m0WX9Rv_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, zipfile, glob, random, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def maybe_unzip(root):\n",
        "    zips = glob.glob(os.path.join(root, \"*.zip\"))\n",
        "    if zips:\n",
        "        for z in zips:\n",
        "            print(\"Unzipping:\", z)\n",
        "            with zipfile.ZipFile(z, 'r') as zip_ref:\n",
        "                zip_ref.extractall(root)\n",
        "\n",
        "maybe_unzip(DATASET_DIR)\n",
        "candidates = [\n",
        "    DATASET_DIR,\n",
        "    os.path.join(DATASET_DIR, \"chest_xray\"),\n",
        "]\n",
        "\n",
        "def has_split(root):\n",
        "    return all(os.path.isdir(os.path.join(root, s)) for s in [\"train\",\"val\",\"test\"])\n",
        "\n",
        "ROOT = None\n",
        "for c in candidates:\n",
        "    if has_split(c):\n",
        "        ROOT = c\n",
        "        break\n",
        "\n",
        "if ROOT is None:\n",
        "    for c in candidates:\n",
        "        if all(os.path.isdir(os.path.join(c, s)) for s in [\"train\",\"test\"]):\n",
        "            ROOT = c\n",
        "            val_dir = os.path.join(ROOT, \"val\")\n",
        "            os.makedirs(val_dir, exist_ok=True)\n",
        "            for cls in os.listdir(os.path.join(ROOT, \"train\")):\n",
        "                src = os.path.join(ROOT, \"train\", cls)\n",
        "                dst = os.path.join(val_dir, cls)\n",
        "                os.makedirs(dst, exist_ok=True)\n",
        "                files = [f for f in glob.glob(os.path.join(src, \"*\")) if os.path.isfile(f)]\n",
        "                random.shuffle(files)\n",
        "                take = max(1, int(0.1*len(files)))\n",
        "                for f in files[:take]:\n",
        "                    shutil.move(f, os.path.join(dst, os.path.basename(f)))\n",
        "            break\n",
        "\n",
        "if ROOT is None:\n",
        "    classes = [d for d in os.listdir(DATASET_DIR) if os.path.isdir(os.path.join(DATASET_DIR, d))]\n",
        "    if set(map(str.lower, classes)) >= {\"normal\",\"pneumonia\"}:\n",
        "        ROOT = os.path.join(DATASET_DIR, \"_split\")\n",
        "        if not os.path.exists(ROOT):\n",
        "            print(\"Creating train/val/test splits (80/10/10) from flat class folders...\")\n",
        "            for split in [\"train\",\"val\",\"test\"]:\n",
        "                for cls in classes:\n",
        "                    os.makedirs(os.path.join(ROOT, split, cls), exist_ok=True)\n",
        "            for cls in classes:\n",
        "                files = [f for f in glob.glob(os.path.join(DATASET_DIR, cls, \"*\")) if os.path.isfile(f)]\n",
        "                random.shuffle(files)\n",
        "                n = len(files)\n",
        "                n_train = int(0.8*n); n_val = int(0.1*n)\n",
        "                for i,f in enumerate(files):\n",
        "                    if i < n_train:\n",
        "                        dst = os.path.join(ROOT, \"train\", cls, os.path.basename(f))\n",
        "                    elif i < n_train + n_val:\n",
        "                        dst = os.path.join(ROOT, \"val\", cls, os.path.basename(f))\n",
        "                    else:\n",
        "                        dst = os.path.join(ROOT, \"test\", cls, os.path.basename(f))\n",
        "                    shutil.copy2(f, dst)\n",
        "\n",
        "assert ROOT is not None, \"Could not detect a valid dataset structure. Ensure folders are one of the expected layouts.\"\n",
        "print(\"Detected dataset root with splits:\", ROOT)\n",
        "!find \"$ROOT\" -maxdepth 2 -type d -print\n"
      ],
      "metadata": {
        "id": "gFb862RO9sLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "train_ds = datasets.ImageFolder(os.path.join(ROOT,\"train\"), transform=train_tfms)\n",
        "val_ds   = datasets.ImageFolder(os.path.join(ROOT,\"val\"),   transform=val_tfms)\n",
        "test_ds  = datasets.ImageFolder(os.path.join(ROOT,\"test\"),  transform=val_tfms)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "class_names = train_ds.classes\n",
        "len(train_ds), len(val_ds), len(test_ds), class_names\n"
      ],
      "metadata": {
        "id": "h1ideEdY91Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XYmJ-bZhcimz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "targets = [y for _, y in train_ds.samples]\n",
        "counts = Counter(targets)\n",
        "num_classes = len(class_names)\n",
        "total = sum(counts.values())\n",
        "class_weights = torch.zeros(num_classes, dtype=torch.float)\n",
        "for c in range(num_classes):\n",
        "    class_weights[c] = total / (num_classes * counts[c])\n",
        "print(\"Class counts:\", counts)\n",
        "print(\"Class weights:\", class_weights)\n"
      ],
      "metadata": {
        "id": "Ui9CD1Qr94ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt, numpy as np, torch, os\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "plt.figure(); plt.plot(history[\"train_loss\"], label=\"train_loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training Loss\"); plt.legend(); plt.savefig(os.path.join(DRIVE_DIR,\"training_loss.png\")); plt.show()\n",
        "plt.figure(); plt.plot(history[\"val_acc\"], label=\"val_acc\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Validation Accuracy\"); plt.legend(); plt.savefig(os.path.join(DRIVE_DIR,\"val_accuracy.png\")); plt.show()\n",
        "\n",
        "state_dict_path = os.path.join(DRIVE_DIR, f\"best_{ARCH}.pth\")\n",
        "model.load_state_dict(torch.load(state_dict_path, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "all_labels, all_preds, all_probs = [], [], []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(DEVICE)\n",
        "        outputs = model(images)\n",
        "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "        preds = np.argmax(probs, axis=1)\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_preds.extend(preds)\n",
        "        if len(class_names)==2:\n",
        "            all_probs.extend(probs[:,1])\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "import itertools\n",
        "plt.figure()\n",
        "plt.imshow(cm, interpolation='nearest'); plt.title('Confusion matrix'); plt.colorbar()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names, rotation=45); plt.yticks(tick_marks, class_names)\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, f\"{cm[i,j]}\", ha=\"center\", va=\"center\", color=\"white\" if cm[i,j] > thresh else \"black\")\n",
        "plt.ylabel('True label'); plt.xlabel('Predicted label'); plt.tight_layout()\n",
        "plt.savefig(os.path.join(DRIVE_DIR, \"confusion_matrix.png\")); plt.show()\n",
        "\n",
        "if len(class_names)==2 and len(all_probs)==len(all_labels):\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "    print(\"ROC AUC:\", auc)\n",
        "    plt.figure(); plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\"); plt.plot([0,1],[0,1],'--'); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curve\"); plt.legend(loc=\"lower right\"); plt.savefig(os.path.join(DRIVE_DIR,\"roc_curve.png\")); plt.show()\n"
      ],
      "metadata": {
        "id": "u8un8KfA-FaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image, ImageFilter, ImageEnhance\n",
        "\n",
        "# Reuse the same normalization as training\n",
        "base_tfms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def pil_from_tensor(x):\n",
        "    # x: (C,H,W) in [0,1] (before normalization)\n",
        "    x = x.clone().cpu()\n",
        "    # assume already un-normalized, or you load fresh from path\n",
        "    np_img = np.transpose(x.numpy(), (1, 2, 0))\n",
        "    np_img = np.clip(np_img, 0, 1)\n",
        "    return Image.fromarray((np_img * 255).astype(np.uint8))\n",
        "\n",
        "def apply_perturbation(img_pil, kind, severity):\n",
        "    img = img_pil.copy()\n",
        "    if kind == \"gaussian_noise\":\n",
        "        np_img = np.array(img)/255.0\n",
        "        noise = np.random.normal(0, severity, np_img.shape)\n",
        "        np_img = np.clip(np_img + noise, 0, 1)\n",
        "        img = Image.fromarray((np_img*255).astype(np.uint8))\n",
        "    elif kind == \"blur\":\n",
        "        img = img.filter(ImageFilter.GaussianBlur(radius=severity))\n",
        "    elif kind == \"brightness\":\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(severity)  # <1 darker, >1 brighter\n",
        "    elif kind == \"rotation\":\n",
        "        img = img.rotate(severity)\n",
        "    # add others if you want: contrast, translation, etc.\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "JDjeW0Q2ngyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# Define what you want to test\n",
        "perturbation_configs = [\n",
        "    (\"none\",        None,      0),\n",
        "    (\"gaussian_noise\", \"gaussian_noise\", 0.05),\n",
        "    (\"gaussian_noise\", \"gaussian_noise\", 0.10),\n",
        "    (\"blur\",       \"blur\",     2),\n",
        "    (\"brightness\", \"brightness\", 0.5),\n",
        "    (\"brightness\", \"brightness\", 1.5),\n",
        "    (\"rotation\",   \"rotation\", 5),\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "# We'll also store baseline predictions to measure flips\n",
        "all_clean_preds = []\n",
        "\n",
        "# First pass: get baseline preds\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = model(images)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        preds = probs.argmax(dim=1)\n",
        "        all_clean_preds.append(preds.cpu())\n",
        "\n",
        "all_clean_preds = torch.cat(all_clean_preds)\n"
      ],
      "metadata": {
        "id": "hMyPm3FUit2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "for name, kind, severity in perturbation_configs:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    changed = 0\n",
        "    idx_global = 0  # track index to compare to clean preds\n",
        "    preds_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=f\"Perturbation: {name}-{severity}\"):\n",
        "            bs = images.size(0)\n",
        "            # Convert each image to PIL and perturb\n",
        "            pil_imgs = []\n",
        "            for i in range(bs):\n",
        "                # un-normalize first:\n",
        "                img = images[i].cpu()\n",
        "                # undo normalization\n",
        "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
        "                std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
        "                img_unnorm = img * std + mean\n",
        "                pil = pil_from_tensor(img_unnorm)\n",
        "                if kind is not None:\n",
        "                    pil = apply_perturbation(pil, kind, severity)\n",
        "                pil_imgs.append(pil)\n",
        "\n",
        "            # Re-apply transforms & stack\n",
        "            pert_tensors = torch.stack([base_tfms(p) for p in pil_imgs]).to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            outputs = model(pert_tensors)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            preds = probs.argmax(dim=1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += bs\n",
        "\n",
        "            # Compare to clean preds for flip rate\n",
        "            clean_batch = all_clean_preds[idx_global:idx_global+bs]\n",
        "            changed += (preds.cpu() != clean_batch).sum().item()\n",
        "            idx_global += bs\n",
        "\n",
        "    acc = correct / total\n",
        "    flip_rate = changed / total\n",
        "    results[(name, severity)] = (acc, flip_rate)\n",
        "    print(f\"{name} (sev={severity}): accuracy={acc:.3f}, flip rate={flip_rate:.3f}\")\n"
      ],
      "metadata": {
        "id": "CqTWUJ1vixjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Robustness Summary ===\")\n",
        "print(\"Perturbation\\tSeverity\\tAccuracy\\tFlipRate\")\n",
        "for (name, sev), (acc, flip) in results.items():\n",
        "    print(f\"{name}\\t{sev}\\t{acc:.3f}\\t{flip:.3f}\")\n"
      ],
      "metadata": {
        "id": "FnySwPLFi1wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "target_layer = model.layer4[-1]  # for ResNet18 / ResNet50\n"
      ],
      "metadata": {
        "id": "qpL4egWoi35g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_NAMES = [\"NORMAL\", \"PNEUMONIA\"]\n",
        "\n",
        "def gradcam_on_image(img_path, target_class_idx=None):\n",
        "    # 1) Load and preprocess image\n",
        "    img_pil = Image.open(img_path).convert(\"RGB\")\n",
        "    img_resized = img_pil.resize((224, 224))\n",
        "    rgb_img = np.array(img_resized) / 255.0  # for overlay (H,W,3, 0â€“1)\n",
        "\n",
        "    input_tensor = base_tfms(img_pil).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # 2) Get model prediction to pick default target class\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(input_tensor)\n",
        "        pred_idx = out.argmax(dim=1).item()\n",
        "\n",
        "    class_idx = target_class_idx if target_class_idx is not None else pred_idx\n",
        "\n",
        "    # 3) Run Grad-CAM (no 'use_cuda' argument in new API)\n",
        "    with GradCAM(model=model, target_layers=[target_layer]) as cam:\n",
        "        grayscale_cam = cam(\n",
        "            input_tensor=input_tensor,\n",
        "            targets=[ClassifierOutputTarget(class_idx)]\n",
        "        )[0]  # [H,W]\n",
        "\n",
        "    # 4) Overlay heatmap on image\n",
        "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    # 5) Plot original + Grad-CAM\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_pil)\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(cam_image)\n",
        "    plt.title(f\"Grad-CAM: {CLASS_NAMES[class_idx]}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return pred_idx, class_idx, grayscale_cam\n"
      ],
      "metadata": {
        "id": "XCj-gKpEi-Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = \"/content/drive/MyDrive/chest_xray/test/PNEUMONIA/person1_virus_6.jpeg\"\n",
        "pred_idx, class_idx, grayscale_cam = gradcam_on_image(img_path)\n",
        "print(\"Model predicted:\", CLASS_NAMES[pred_idx])\n"
      ],
      "metadata": {
        "id": "BxHpBwshi_pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def central_focus_score(grayscale_cam, central_frac=0.5):\n",
        "    h, w = grayscale_cam.shape\n",
        "    ch = int(h * central_frac)\n",
        "    cw = int(w * central_frac)\n",
        "    y0 = (h - ch)//2\n",
        "    x0 = (w - cw)//2\n",
        "\n",
        "    central = grayscale_cam[y0:y0+ch, x0:x0+cw]\n",
        "    total = grayscale_cam.sum() + 1e-8\n",
        "    return central.sum() / total\n",
        "\n",
        "# Example using the grayscale_cam inside gradcam_on_image\n"
      ],
      "metadata": {
        "id": "XzndqZJ-jBpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = \"/content/drive/MyDrive/chest_xray/test/PNEUMONIA/person1_virus_6.jpeg\"\n",
        "\n",
        "pred_idx, class_idx, grayscale_cam = gradcam_on_image(img_path)\n",
        "score = central_focus_score(grayscale_cam, central_frac=0.5)\n",
        "\n",
        "print(\"Model predicted:\", CLASS_NAMES[pred_idx])\n",
        "print(\"Central focus score:\", score)\n"
      ],
      "metadata": {
        "id": "k9cfU8yrjDiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3urbOgV7md5_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}